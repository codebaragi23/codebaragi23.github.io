---
title: 1. VGGNet 논문 리뷰
excerpt: "논문명: Very Deep Convolutional Networks for Large-Scale Image Recognition"
categories: 
 - Machine Learning
tags: 
 - VGGNet
 - VGG16
 - 논문 리뷰

#excerpt: 

header:
 teaser: https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9982C64E5BF9048B01&auto=format&fit=crop&w=256&q=40
 overlay_image: https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9982C64E5BF9048B01&auto=format&fit=crop&w=256&q=40
 overlay_filter: 0.5
 #caption: Photo by Luca Bravo on Unsplash

icon: fa-github
---

논문 링크 : <https://arxiv.org/pdf/1409.1556.pdf>

## Abstract

VGGNet은 ILSVRC 2014 대회에서 2등을 차지한 Karen Simonyan과 Andrew Zisserman이 만든 CNN 모델로 네트워크의 깊이가 모델이 좋은 성능을 보이는 데 중요한 역할을 한다는 것을 보여줬다.
VGGNet의 필터 크기는 3x3, stride 1, 제로 패딩 1의 Conv 레이어로 이루어져 있으며, 필터 크기 2x2 (패딩 없음)의 Max-pool을 Pooling 레이어로 사용한다.
ILSVRC 대회에서는 GoogLeNet 보다 이미지 분류 성능은 낮았지만, 다른 연구에서는 좋은 성능을 보인다. (논문의 Appendix에서 확인 가능)
최근에는 이미지 특징(feature)을 추출하는 데 이용되는 등 기본 네트워크 모델로 활용되고 있으며, 매우 많은 메모리를 이용하여 연산한다는 단점이 있다.

---


## 1. Introduction

컨볼루션 네트워크 연구는 2012년 AlexNet을 통해 영상 및 비디오 인식 결과에서 뛰어난 결과를 보이고 더욱 좋은 결과를 내기 위하여 다양한 연구들이 진행되어 왔다. 본 논문에서는 컨볼루션 네트워크 구조의 깊이에 따른 인식 결과에 대해 설명하려고 한다.


## 2. ConvNet Configurations

VGG 모델은 딥러닝 기반 컴퓨터 비전 모델의 시대를 열었던 AlexNet(2012)의 8-layers 모델보다 깊이가 2배 이상 깊은 네트워크의 학습에 성공했으며, 이를 통해 ImageNet Challenge에서 AlexNet의 오차율을 절반(16.4 > 7.3)으로 줄였다.
VGG 모델이 16–19 레이어에 달하는 깊은 신경망을 학습할 수 있었던 것은 모든 합성곱 레이어에서 3x3 필터를 사용했기 때문이다.

[![나타낼 수 없음](https://www.researchgate.net/profile/Hirokatsu_Kataoka/publication/282270749/figure/fig1/AS:613445221232696@1523268146609/AlexNet-and-VGGNet-architecture.png)](https://www.researchgate.net/profile/Hirokatsu_Kataoka/publication/282270749/figure/fig1/AS:613445221232696@1523268146609/AlexNet-and-VGGNet-architecture.png)

[![나타낼 수 없음](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)

VGG-16 Architecture의 구성
- 13 Convolution Layers + 3 Fully-connected Layers
- 3x3 convolution filters
- stride: 1 & padding: 1
- 2x2 max pooling (stride : 2)
- ReLU

아래 표는 그들이 실험에 사용한 6개의 구조를 보여준다.

[![나타낼 수 없음](https://miro.medium.com/max/2200/0*HREIJ1hjF7z4y9Dd.jpg)](https://miro.medium.com/max/2200/0*HREIJ1hjF7z4y9Dd.jpg)

입력 사이즈는 224x224 크기의 컬러 영상을 사용하며, 하나 또는 복수의 컨볼루션층과 Max-pooling층이 반복되는 구조이고 최종단에는 Fully connected layer가 구성되어 있다. 전처리 단계에서의 AlexNet과 차이점은 학습 데이터셋 전체의 채널 평균 값을 입력 영상의 각 화소마다 빼주고 입력을 zero-centered되게 한다. VGGNet에서는 3x3크기의 필터를 사용함으로써, 여러개의 ReLU non-linearity의 사용을 증가시킬 수 있다고 한다. 그리고 기존의 7x7 사이즈보다 작은 크기의 필터를 사용함으로써 상당수의 파라미터를 줄일 수 있었다. 그럼에도 불구하고 최종단의 Fully connected layer로 인해 상당수의 파라미터가 존재하고 있다.


VGGNet의 단점은 GoogLeNet 저자 Szegedy가 비판을 했던 것처럼, 파라미터의 개수가 너무 많다는 점이다. 위의 표를 보면 알 수 있는 것처럼, GoogLeNet의 파라미터의 개수가 5 백만개 수준이었던 것에 비해 VGGNet은 가장 단순한 A-구조에서도 파라미터의 개수가 133 백만개로 엄청나게 많다.

그 결정적인 이유는 VGGNet의 경우는 AlexNet과 마찬가지로 최종단에 fully-connected layer 3개가 오는데 이 부분에서만 파라미터의 개수가 약 122 백만개가 온다. 참고로 GoogLeNet은 Fully-connected layer가 없다.


---

**NOTE**

3x3 필터 사용

VGG 모델 이전에 Convolutional Network를 활용하여 이미지 분류에서 좋은 성과를 보였던 모델들은 비교적 큰 Receptive Field를 갖는 11x11필터나 7x7 필터를 포함한다.
그러나 VGG 모델은 오직 3x3 크기의 작은 필터만 사용했음에도 이미지 분류 정확도를 비약적으로 개선시켰다. 이 부분에서 좋은 통찰을 얻을 수 있다.

여기서 Receptive Field란?

영상에서 특정 위치에 있는 픽셀들은 그 주변에 있는 일부 픽셀들 하고만 correlation이 높고 거리가 멀어질수록 그 영향은 감소하게 된다. 이를 이용해 영상이나 이미지를 해석하여 "인식 알고리즘"을 수행하고자 할 경우 영상 전체 영역에 대해 서로 동일한 중요도를 부여하여 처리하는 대신에 특정 범위를 한정해 처리를 하면 훨씬 효과적일 것이라 짐작 할 수 있다. 이를 영상에만 한정하는 것이 아니라 locality를 갖는 모든 신호들에 유사하게 적용할 수있다는 아이디어에 기반하여 출현한 것이 CNN이다.


이어서 7x7필터와 3x3필터로 각각 Convolution을 수행하면 다음과 같은 특성이 있다.

![나타낼 수 없음](https://miro.medium.com/max/2554/1*Cb8p7EzcWYDHUzMBYI-yyw.png)  
_7x7 필터를 이용하여 Convolution 했을 경우 출력 특징 맵의 각 픽셀 당 Receptive Field는 7x7이다._

![나타낼 수 없음](https://miro.medium.com/max/2546/1*E9DiwjWyLU-aQU-knOtv3g.png)  
_3x3필터를 이용할 경우 3-레이어 Convolution을 반복했을 때 원본 이미지의 7x7 영역을 수용할 수 있다._

**Stride가 1일 때, 3차례의 3x3 Conv 필터링을 반복한 특징맵은 한 픽셀이 원본 이미지의 7x7 Receptive field의 효과를 볼 수 있다.**

그렇다면, 7x7 필터를 이용해 이미지에 대해 한 번 Convolution을 수행한 것과 3x3 필터로 세 번 Convolution을 수행한 것에는 어떤 차이가 있을까?

 1. 결정 함수의 비선형성 증가  
  각 Convolution 연산은 ReLU 함수를 포함한다. 다시 말해, 1-layer 7x7 필터링의 경우 한 번의 비선형 함수가 적용되는 반면 3-layer 3x3 필터링은 세 번의 비선형 함수가 적용된다.
  따라서, 레이어가 증가함에 따라 비선형성이 증가하게 되고 이것은 모델의 특징 식별성 증가로 이어진다.
 2. 학습 파라미터 수의 감소

  Convolutional Network 구조를 학습할 때, 학습 대상인 가중치(weight)는 필터의 크기에 해당한다.

  따라서,  **7x7필터 1개에 대한 학습 파라미터 수는 49**이고  **3x3 필터 3개에 대한 학습 파라미터 수는 27(3x3x3)**이 된다.

  파라미터 수가 크게 감소하는 것을 알 수 있다.

물론 위와 같은 특징이 모든 경우에 좋은 방향으로 작용하는 것은 아니니 주의할 필요가 있다. 다시 말해, 무작정 네트워크의 깊이를 깊게 만드는 것이 장점만 있는 것은 아니다.

여러 레이어를 거쳐 만들어진 특징 맵(Feature Map)은 동일한 Receptive Field에 대해 더 추상적인 정보를 담게 된다. 목적에 따라서는 더 선명한 특징 맵이 필요할 수도 있다.

---

## 3. Classification Framework

VGGNet 팀은 3x3 convolution이라는 단순한 구조로부터 성능을 끌어내기 위해, training과 test에 많은 공을 들였으며, 다양한 경우에 성능이 어떻게 달라지는지 공개하여 CNN 구조 이해에 많은 기여를 하였다.

### 3.1 Training

원문에서는 학습 시 다음과 같은 최적화 알고리즘을 사용하였다

- Optimizing multinomial logistic regression
- mini-batch gradient descent
- Momentum(0.9)
- Weight Decay(L2 Norm)
- Dropout(0.5)
- Learning rate 0.01로 초기화 후 서서히 줄임

**가중치 초기화**

딥러닝에서 신경망 가중치의 초기화는 학습 속도 및 안정성에 큰 영향을 줄 수 있기 때문에 어떤 방식으로 초기화할 것인지는 중요한 문제 중 하나이다. VGG 연구팀은 이러한 문제를 보완하고자 다음과 같은 전략을 세웠다.

- 상대적으로 얕은 11-Layer 네트워크를 우선적으로 학습한다. 이 때, 가중치는 정규분포를 따르도록 임의의 값으로 초기화한다.
- 어느 정도 학습이 완료되면 입력층 부분의 4개 층과 마지막 3개의 fully-connected layer의 weight를 학습할 네트워크의 초기값으로 사용한다.

> 논문 제출 후 Glorot&Bengio (2010)의 무작위 초기화 절차를 이용하여 사전 훈련 없이 가중치를 초기화하는 것이 가능하다는 것을 알아냈다.

**학습 이미지 크기**

모델 학습(Training) 시 입력 이미지의 크기는 모두 224x224로 고정하였다.

AlexNet, GooLeNet과 비교해 보면,

AlexNet은 모든 학습 이미지를 256x256 크기로 만든 후, 거기서 무작위로 224x224 크기의 이미지를 취하는 방식으로 학습 데이터의 크기를 2048배로 늘렸으며, RGB 컬러를 주성분 분석(PCA)를 사용하여 RGB 데이터를 조작하는 방식도 사용하였다. 하지만 AlexNet에서는 모든 이미지를 256x256 크기의 single scale 만을 사용하였다.

반면에 GoogLeNet은 영상의 가로/세로 비를 [3/4, 4/3]의 범위를 유지하면서 원영상의 8%부터 100%까지 포함할 수 있도록 하여 다양한 크기의 patch를 학습에 사용하였다. 또한 photometric distortion을 통해 학습 데이타를 늘렸다.

VGGNet에서는 **training scale**을 ‘S’로 표시하며, single-scale training과 multi-scaling training을 지원한다. Single scale에서는 AlexNet과 마찬가지로 S = 256, 또는 S = 384 두개의 scale 고정을 지원한다.

Multi-scale의 경우는 S를 Smin과 Smax 범위에서 무작위로 선택할 수 있게 하였으며, Smin은 256이고 Smax는 512이다. 즉, 256과 512 범위에서 무작위로 scale을 정할 수 있기 때문에 다양한 크기에 대한 대응이 가능하여 정확도가 올라간다. Multi-scale 학습은 S = 384로 미리 학습 시킨 후 S를 무작위로 선택해가며 fine tuning을 한다. S를 무작위로 바꿔 가면서 학습을 시킨다고 하여, 이것을 **scale jittering**이라고 하였다.

이렇게 크기 조정을 통해 얻은 학습 영상으로부터 AlexNet과 마찬가지 방식으로 무작위로 224x224 크기를 선택하였으며, RGB 컬러 성분에 대한 변경 역시 비슷한 방식으로 수행했다.

GoogLeNet과 VGGNet은 그 이름과 표현만 좀 다를 뿐이지, 결과적으로는 multi-scale을 고려하였고, RGB 컬러 성분 변경을 통해 deep network이 적은 학습 데이터로 인한 overfitting 문제에 빠지는 것을 최대한 막으려는 노력을 하였다.

![나타낼 수 없음](https://miro.medium.com/max/2526/1*GgBEeC19t2BEEYTTnC9zrA.png)

![나타낼 수 없음](https://miro.medium.com/max/2290/1*GoMqM5gklSZdgbzek-bxLg.png)

![나타낼 수 없음](https://miro.medium.com/max/2538/1*j6IOqW5REUoL7yL4xFGZMw.png)  
_이미지를 256x256 크기로 변환 후 224x224 크기를 샘플링한 경우_

![나타낼 수 없음](https://miro.medium.com/max/2538/1*FfnOsvhhdJQDyHg9PjrKFA.png)  
_이미지를 512x512 크기로 변환후 224x224 크기를 샘플링한 경우_

이처럼 학습 데이터를 다양한 크기로 변환하고 그 중 일부분을 샘플링해 사용함으로써 몇 가지 효과를 얻을 수 있다.

1. 한정적인 데이터의 수를 늘릴 수 있다. — Data augmentation
2. 하나의 오브젝트에 대한 다양한 측면을 학습 시 반영시킬 수 있다. 변환된 이미지가 작을수록 개체의 전체적인 측면을 학습할 수 있고, 변환된 이미지가 클수록 개체의 특정한 부분을 학습에 반영할 수 있다.

두 가지 모두 Overfitting을 방지하는 데 도움이 된다.

실제로 VGG 연구팀의 실험 결과에 따르면 다양한 스케일로 변환한 이미지에서 샘플링하여 학습 데이터로 사용한 경우가 단일 스케일 이미지에서 샘플링한 경우보다 분류 정확도가 좋았다.

### 3.2 Testing

Training 완료된 모델을 테스팅할 때는 신경망의 마지막 3 Fully-Connected layers를 Convolutional layers로 변환하여 사용하였다.

첫 번째 Fully-Connected layer는 7x7 Conv로, 마지막 두 Fully-Connected layer는 1x1 Conv로 변환하였다. 이런식으로 변환된 신경망을 Fully-Convolutional Networks라 부른다.

신경망이 Convolution 레이어로만 구성될 경우 입력 이미지의 크기 제약이 없어진다. 이에 따라 하나의 입력 이미지를 다양한 스케일로 사용한 결과들을 앙상블하여 이미지 분류 정확도를 개선하는 것도 가능해진다.

Fully-Convolutional Network는 컴퓨터 비전 분야에서 굉장히 중요한 개념이다.

## 4. Classification Experiments

### 4.1 Single-Scale Evaluation

VGGNet에 single-scale test 영상을 적용했을 때의 결과는 아래 표와 같다. 망이 깊어질수록 결과가 좋아지고, 학습에 scale jittering을 사용한 경우에 결과가 더 좋다는 것을 확인할 수 있다.

3x3 convolution layer 2개를 겹치면 5x5 convolution의 효과를 얻을 수 있다는 것은 이미 앞선 class에서 살펴봤다. 둘 중 어느 결과가 좋을까?

B 구조에 3x3 convolution layer를 2개 겹쳐서 사용하는 경우와 5x5 convolution 1개를 사용하는 모델을 만들어 실험을 했는데, 결과는 3x3 2개를 사용하는 것이 5x5 1개보다 top-1 error에서 7% 정도 결과가 좋았다고 한다. 3x3 convolution이 단순하게 망을 깊게 만들고, 파라미터의 크기를 줄이는 것뿐만 아니라, 뉴런에 있는 non-linearity 활성함수를 통해 feature 추출 특성이 좋아졌음을 반증한다.

[![나타낼 수 없음](https://postfiles.pstatic.net/20160630_289/laonple_1467259172054sspYy_PNG/%C0%CC%B9%CC%C1%F6_43.png?type=w2)](https://postfiles.pstatic.net/20160630_289/laonple_1467259172054sspYy_PNG/%C0%CC%B9%CC%C1%F6_43.png?type=w2)

### 4.2 Multi-Scale Evaluation

Multi-scale test 결과는 아래 표와 같다. Multi-scale test는 S가 고정된 경우는 {S-32, S, S+32}로 Q 값을 변화 시키면서 테스트를 했다. 여기서 학습의 scale과 테스트의 scale이 많이 차이가 나는 경우는 오히려 결과가 더 좋지 못해 32만큼 차이가 나게 하여 테스트를 하였다.

학습에 scale jittering을 적용한 경우는 출력의 크기는 [256, 384, 512]로 테스트 영상의 크기를 정했으며, 예상처럼 scale jittering을 적용하지 않은 것보다 훨씬 결과가 좋고, single-scale 보다는 multi-scale이 결과가 좋다는 것을 확인할 수 있다.

[![나타낼 수 없음](https://postfiles.pstatic.net/20160630_161/laonple_1467259323970DuKM3_PNG/%C0%CC%B9%CC%C1%F6_46.png?type=w2)](https://postfiles.pstatic.net/20160630_161/laonple_1467259323970DuKM3_PNG/%C0%CC%B9%CC%C1%F6_46.png?type=w2)

### 4.3 MULTI-CROP EVALUATION

앞서 살펴본 것처럼, multi-crop과 dense evaluation은 각각 적용했을 때는 grid 크기 문제로 인해 multi-crop이 아주 약간 성능이 좋은 편이며, 상보적인 특성을 갖고 있기 때문에 같이 적용을 하게 되면 성능이 개선되는 것을 아래 표처럼 알 수가 있다.

![나타낼 수 없음](https://postfiles.pstatic.net/20160630_120/laonple_14672591723355hgpI_PNG/%C0%CC%B9%CC%C1%F6_45.png?type=w2)

 
## 5. Conclusion

결론적으로 VGGNet은 그 구조가 간단하여 이해나 변형이 쉬운 분명한 장점을 갖고 있기는 하지만, 파라미터의 수가 엄청나게 많기 때문에 학습 시간이 오래 걸린다는 분명한 약점을 갖고 있다.

실험에서 네트워크의 깊이를 최대 19 레이어(VGG-19)까지만 사용한 이유는 해당 실험의 데이터에서는 분류 오차율이 VGG-19에서 수렴했기 때문이다. 학습 데이터 세트가 충분히 많다면 더 깊은 모델이 더 유용할 수도 있다.


**Reference**

- [라온피플's blog](https://blog.naver.com/laonple/220643128255)
- [강준영's medium](https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a)

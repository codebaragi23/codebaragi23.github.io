---
title: 7.4 요약
---

- 이 장에서 다음 내용을 배웠습니다.
- 임의의 층 그래프를 구성하는 모델을 만드는 방법, 층을 재사용하는 방법(가중치 공유), 파이썬 함수 방식으로 모델을 사용하는 방법(모델 템플릿)
- 케라스 콜백을 사용하여 훈련하는 동안 모델을 모니터링하고 모델 상태를 바탕으로 작업을 수행합니다.
- 텐서보드를 사용하여 측정 지표, 활성화 출력의 히스토그램, 임베딩 공간을 시각화합니다.
- 배치 정규화, 깊이별 분리 합성곱, 잔차 연결
- 하이퍼파라미터 최적화와 모델 앙상블을 사용하는 이유
- 이런 새로운 도구를 활용하면 실전 문제에 딥러닝을 더 잘 적용하고 경쟁력 높은 딥러닝 모델을 만들 수 있습니다.


[22](Section0702.html#footnote-07-22-backlink) 역주 케라스 모델은 fit() 메서드가 반환하는 history 객체를 위한 History 콜백, 측정 지표의 평균을 계산하는 BaseLogger 콜백, fit() 메서드에 verbose=0을 지정하지 않았다면 진행 표시줄을 위한 ProgbarLogger 콜백이 자동으로 추가됩니다.

[23](Section0702.html#footnote-07-23-backlink) 역주 monitor 매개변수의 기본값은 ‘val_loss’입니다.

[24](Section0702.html#footnote-07-24-backlink) 역주 검증 정확도를 측정하려면 compile() 메서드의 metrics 매개변수에 ‘acc’를 포함하고 fit() 메서드의 validation_data 매개변수에 검증 데이터를 전달해야 합니다.

[25](Section0702.html#footnote-07-25-backlink) 역주 validation_data의 첫 번째 원소는 입력 데이터고, 두 번째 원소는 레이블입니다.

[26](Section0702.html#footnote-07-26-backlink) 역주 Embeddings 탭의 왼쪽 아래 패널에서 t-SNE와 PCA를 선택할 수 있습니다. t-SNE 그래프는 2D, 3D를 선택할 수 있고 PCA는 3개의 주성분을 추출합니다. t-SNE와 PCA에 대한 자세한 설명은 <파이썬 라이브러리를 활용한 머신러닝>(한빛미디어, 2017)의 3장을 참고하세요.

[27](Section0702.html#footnote-07-27-backlink) 역주 plot_model() 함수에는 2개의 매개변수가 더 있습니다. show_layer_names=True로 지정하면 층 이름을 포함합니다. rankdir= 'TB'는 수직으로 그래프를 그리고(기본값입니다), ‘LR’은 수평 그래프를 만듭니다.

[28](Section0703.html#footnote-07-28-backlink) Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” Proceedings of the 32nd International Conference on Machine Learning (2015), [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).

[29](Section0703.html#footnote-07-29-backlink) 역주 배치 정규화는 입력 배치의 평균과 표준 편차를 지수 이동 평균으로 계산하여 전체 데이터셋의 평균과 표준 편차를 대신합니다. 이 값은 테스트 데이터에 배치 정규화가 적용될 때 사용됩니다. 지수 이동 평균은 3장에서 본 것처럼 v = v × momentum + v_new × (1 - momentum)으로 계산됩니다. momentum이 클수록 이전 값(v)의 관성이 크며 새로운 값(v_new)이 미치는 영향이 적습니다. 케라스의 BatchNormalization 클래스의 momentum 기본값은 0.99입니다.

[30](Section0703.html#footnote-07-30-backlink) 역주 입력에 비하여 활성화 함수의 출력이 너무 작거나 커지면 변화율이 급격히 작아져 (그림 3-5의 시그모이드 함수 참고) 역전파되는 그래디언트도 매우 줄어들게 됩니다. 배치 정규화는 입력과 출력의 분포를 유지하도록 도와주므로 그래디언트가 더 잘 전파됩니다.

[31](Section0703.html#footnote-07-31-backlink) 역주 0번째 축은 항상 배치 차원입니다.

[32](Section0703.html#footnote-07-32-backlink) Sergey Ioffe, “Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models” (2017), [https://arxiv.org/abs/1702.03275](https://arxiv.org/abs/1702.03275).

[33](Section0703.html#footnote-07-33-backlink) Günter Klambauer et al., “Self-Normalizing Neural Networks,” Conference on Neural Information Processing Systems (2017), [https://arxiv.org/abs/1706.02515](https://arxiv.org/abs/1706.02515).

[34](Section0703.html#footnote-07-34-backlink) 역주 initializers.lecun_normal() 함수는 입력 유닛 개수의 역수에 대한 제곱근을 표준 편차로 하는 절단 정규 분포로 가중치를 초기화하는 방법입니다. 이 방법은 얀 르쿤(Yann LeCun)이 소개했습니다([https://bit.ly/2DeQC3Q](https://bit.ly/2DeQC3Q)).

[35](Section0703.html#footnote-07-35-backlink) 주석 13번을 참고하세요.

[36](Section0703.html#footnote-07-36-backlink) 역주 케라스의 keras.wrappers.scikit_learn 모듈 아래에 있는 KerasClassifier와 KerasRegressor 클래스를 이용하면 사이킷런의 RandomizedSearchCV를 사용하여 랜덤한 하이퍼파라미터 탐색을 수행할 수 있습니다.

[37](Section0703.html#footnote-07-37-backlink) 역주 Hyperopt는 베이지안 최적화 도구 중 하나로 2011년에 발표된([https://bit.ly/2Jru9Tx](https://bit.ly/2Jru9Tx)) Parzen 트리 추정기(Parzen Tree Estimator, PTE)를 사용합니다. 케라스를 위한 하이퍼파라미터 탐색 도구인 Auto-Keras([https://bit.ly/2MmAZfs](https://bit.ly/2MmAZfs))가 최근 공개되었습니다. 사이킷런과 함께 사용할 수 있는 대표적인 하이퍼파라미터 탐색 라이브러리로 TPOT([https://bit.ly/2LhAXVr](https://bit.ly/2LhAXVr))과 auto-sklearn([https://bit.ly/2NT2wX4](https://bit.ly/2NT2wX4))이 있습니다.

[38](Section0703.html#footnote-07-38-backlink) 역주 넬더-미드 방법은 존 넬더(John Nelder)와 로저 미드(Roger Mead)가 1965년에 소개한 비선형 최적화 문제를 위한 방법으로 아메바 방법(amoeba method)이라고도 부릅니다. 이 책의 저자 프랑소와 숄레가 구현한 넬더-미드 알고리즘이 깃허브에 공개되어 있습니다([https://bit.ly/2Ll5jGR](https://bit.ly/2Ll5jGR)).
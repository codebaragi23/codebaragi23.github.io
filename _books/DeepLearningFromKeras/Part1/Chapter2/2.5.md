---
title: 2.5 첫 번째 예제 다시 살펴보기
---

이제 신경망의 이면에 어떤 원리가 있는지 기초적인 내용을 이해했을 것입니다. 첫 번째 예제로 다시 돌아가서 이전 절에서 배웠던 내용을 이용하여 코드를 자세하게 리뷰합시다.

**입력 데이터**

```python
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255
```

- type : float32,
- 훈련 데이터 크기: (60000, 28*28)
- 테스트 데이터 크기: (10000, 28*28) 

**신경망**

```python
network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
network.add(layers.Dense(10, activation='softmax'))
```

- 네트워크에 2개의 Dense 층이 연결
- 각 층은 가중치 텐서를 포함하여 입력 데이터에 대한 몇 개의 간단한 텐서 연산을 적용
- 가중치 텐서는 네트워크가 정보를 저장하는 곳

**네트워크 컴파일 단계**

```python
network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
```

- 손실함수 : `categorical_crossentropy` 
가중치 텐서를 학습하기 위한 피드백 신호로 사용되며 훈련하는 동안 최소화
- 미니 배치 확률적 경사 하강법을 통해 손실이 감소
- 경사 하강법을 적용하는 구체적인 방식은 첫 번째 매개변수로 전달된 `rmsprop` 옵티마이저에 의해 결정

**훈련 반복**

```python
network.fit(train_images, train_labels, epochs=5, batch_size=128)
```

fit 메서드를 호출했을 때 다음과 같은 일이 일어납니다.

- 네트워크가 128개 샘플씩 미니 배치로 훈련 데이터를 다섯 번 반복[^1]
- 각 반복마다 배치에서 네트워크의 가중치에 대한 그래디언트를 계산하여 가중치 업데이트
- 다섯 번의 에포크 동안 네트워크는 2,345번의 그래디언트 업데이트를 수행(에포크마다 469번[^2]).

[^1]: 전체 훈련 데이터에 수행되는 각 반복을 에포크(epoch)라고 합니다
[^2]: 훈련 샘플이 6만 개이므로 128개씩 배치로 나누면 469개의 배치가 만들어지며 마지막 배치의 샘플 개수는 96개가 됩니다.

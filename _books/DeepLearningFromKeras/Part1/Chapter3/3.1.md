---
title: 3.1 신경망의 구조
---

신경망 훈련에는 다음 요소들이 관련되어 있습니다.

- **네트워크**(또는 **모델**)를 구성하는 **층**
- **입력 데이터**와 그에 상응하는 **타깃**
- 학습에 사용할 피드백 신호를 정의하는 **손실 함수**
- 학습 진행 방식을 결정하는 **옵티마이저**

이들 간의 상호 작용을 다시한번 그림 3–1에 나타냈습니다.

![표시 할 수 없음](https://dpzbhybb2pdcj.cloudfront.net/chollet/Figures/03fig01.jpg)  
_그림 3-1. 네트워크, 층, 손실 함수, 옵티마이저 사이의 관계_

다시 한번 정리해 보겠습니다. 연속된 층으로 구성된 네트워크가 입력 데이터를 예측으로 매핑합니다. 손실 함수는 예측과 실제 타깃을 비교하여 네트워크의 정확성을 측정하는 손실 점수를 만듭니다. 옵티마이저는 손실 점수를 사용해 네트워크 가중치를 업데이트합니다.

층, 네트워크, 손실 함수, 옵티마이저에 대해 자세히 살펴보겠습니다.


## 3.1.1 층: 딥러닝 구성 단위

**층**은 하나 이상의 텐서를 입력으로 받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈입니다.

대부분의 층은 **가중치**라는 층의 상태를 가집니다.[^1] 가중치는 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서이며 여기에 네트워크가 학습한 **지식**이 담겨 있습니다.

[^1]: 5장에서 소개될 플랫튼(Flatten), 풀링(Pooling), 드롭아웃(Dropout) 층에는 학습되는 가중치가 없습니다. 보통 신경망의 가중치를 상태라고 표현하지는 않습니다. 6장에서 소개될 순환 신경망의 셀(cell) 출력은 셀의 상태라고 표현합니다.

층마다 적절한 텐서 포맷과 데이터 처리 방식이 다릅니다. 아래의 예를 들어보겠습니다.

- 2D 텐서: **완전 연결 층(fully connected layer)**, **밀집 층(dense layer)**이라 불리는 밀집 연결 층(densely connected layer)(케라스는 Dense 클래스)
- 3D 텐서: LSTM 같은 **순환 층(recurrent layer)**
- 4D 텐서: **2D 합성곱 층(convolution layer)**(Conv2D 클래스).

케라스는 호환 가능한 층들을 엮어 데이터 변환 파이프라인(pipeline)을 구성해 딥러닝 모델을 만들기 때문에 층을 하나의 레고 블록처럼 생각할 수 있습니다. 이를 **층 호환성(layer compatibility)**이라 하며 층의 입력과 출력에 따라 연결 가능 유무가 결정됩니다.

아래의 예를 통해 살펴보겠습니다.

```python
from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(32, input_shape=(784,))) # 32개의 유닛으로 된 밀집 층
model.add(layers.Dense(10))
```

첫 층의 첫 번째 차원이 784이고, 0번째 축은 지정하지 않은 2D 텐서만 입력으로 받는 층을 만들었습니다(지정하지 않으면 어떠한 배치 크기도 상관없습니다[^2]). 또한 출력은 첫 번째 차원 크기가 32로 변환된 텐서입니다.

[^2]: 2장의 예제처럼 배치 크기는 모델의 `fit()` 메서드에서 지정합니다. `LSTM` 같은 순환 신경망(6장 참고)에서는 현재 배치의 셀 상태를 다음번 배치의 셀 상태 초깃값으로 사용하기 위해 `stateful = True` 매개변수를 지정할 수 있습니다. 이때는 `batch_input_shape` 매개변 수를 사용하여 배치 크기가 포함된 입력 텐서의 크기를 지정해야 합니다.

따라서 이 층에는 32차원의 벡터를 입력으로 받는 하위 층이 연결되어야 합니다. 위 예의 두 번째 층에는 `input_shape` 매개변수를 지정하지 않았습니다. 케라스에서는 자동으로 상위 층의 크기에 맞추어 주기 때문에 호환성을 걱정하지 않아도 됩니다.


## 3.1.2 모델: 층의 네트워크

딥러닝 모델은 층으로 만든 **비순환 유향 그래프(Directed Acyclic Graph, DAG)[^3]**입니다. 가장 일반적인 예가 하나의 입력을 하나의 출력으로 매핑하는 층을 순서대로 쌓는 것입니다. 앞으로 아주 다양한 네트워크 구조를 보게 될 것입니다. 자주 등장하는 것들은 다음과 같습니다. [^4]

- 가지(branch)가 2개인 네트워크
- 출력이 여러 개인 네트워크
- 인셉션(Inception) 블록

[^3]: 그래프 이론에서 비순환 유향 그래프는 그래프의 edge에 방향이 있고 한 node에서 다시 자기 자신으로 돌아올 경로가 없는 그래프를 말합니다.
[^4]: 여기에 나온 구조는 7장에서 좀 더 자세히 설명합니다.

1장에서 머신 러닝을 ‘`가능성 있는 공간을 사전에 정의하고 피드백 신호의 도움을 받아 입력 데이터에 대한 유용한 변환을 찾는 것`’으로 정의했습니다.

네트워크 구조를 선택함으로써 **가능성 있는 공간(가설 공간)**을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한됩니다. 우리가 찾아야 할 것은 이런 텐서 연산에 포함된 가중치 텐서의 최적값입니다.

딱 맞는 네트워크 구조를 찾아내는 것은 매우 어려우며 연습을 해야만 적절한 신경망을 설계할 수 있게 될 것입니다. 다음 몇 개의 장에서 신경망을 만드는 원리를 배우고 특정 문제로의 적용 가능여부를 판단하는 직관을 길러 봅시다.


## 3.1.3 손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠

네트워크 구조를 정의하고 나면 두 가지를 더 선택해야 합니다.

- **손실 함수(loss function)**: 훈련하는 동안 최소화될 값. 주어진 문제에 대한 성공 지표. (_목적 함수(objective function), 오차 함수(Error Function)_)
- **옵티마이저(optimizer)**: 손실 함수를 기반으로 네트워크가 업데이트 결정. 특정 종류의 확률적 경사 하강법(SGD)을 구현.

여러 개의 출력을 내는 신경망은 여러 개의 손실 함수를 가질 수 있습니다.[^5] 하지만 경사 하강법 과정은 하나의 스칼라 손실 값을 기준으로 하므로 모든 손실이 (평균을 내서) 하나의 스칼라 양으로 합쳐집니다.

[^5]: compile 메서드의 loss 매개변수에 손실 함수의 리스트 또는 딕셔너리를 전달합니다. [7.1.3절]()에서 이와 관련된 예제를 볼 수 있습니다.

손실 함수의 올바른 선택은 매우 중요합니다. 이는 네트워크가 결과값의 최적화만을 목료로 하기 때문에 의도하지 않은 모델이 만들어 질 수 있습니다.

예로 “모든 인류의 평균 행복 지수를 최대화하기” 같은 잘못된 목적(손실) 함수를 가정합시다. 이 문제를 쉽게 해결하려고 이 네트워크는 대부분의 인류를 죽여 소수 남은 사람들의 행복에만 초점을 맞출지도 모릅니다. 목적인 평균 행복 지수는 얼마나 많은 사람이 남겨져 있는지와 상관없기 때문입니다.

다행히 분류, 회귀와 시퀀스 예측 같은 일반적인 문제에서는 올바른 손실 함수를 선택하는 간단한 지침이 있습니다. 아래의 예를 보겠습니다.

- 2개의 클래스가 있는 분류 문제: **이진 크로스엔트로피(binary cross-entropy)**
- 여러 개의 클래스가 있는 분류 문제: **범주형 크로스엔트로피(categorical cross-entropy)**
- 회귀 문제: **평균 제곱 오차(Mean Squared Error, MSE)**
- 시퀀스 학습 문제: **CTC(Connection Temporal Classification)**

새로운 연구를 할 때만 독자적인 목적 함수를 만들게 될 것입니다. 이어지는 장들에서 여러 분야에서 어떤 손실 함수를 선택하는지 자세히 설명합니다. [^6]

[^6]: 이진 분류를 위한 크로스엔트로피 손실을 로지스틱 손실(Logistic loss) 또는 로그 손실(Log loss)이라고도 부릅니다. 크로스엔트로피 는 3.4.3절, 평균 제곱 오차는 3.6.3절에서 설명합니다. CTC는 음성 인식이나 필기 인식처럼 입력에 레이블 할당 위치를 정하기 어려운 연속 적인 시퀀스를 다루는 문제에 사용하는 손실 함수입니다.
